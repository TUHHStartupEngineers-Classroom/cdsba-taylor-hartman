[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "1 \n\n\nRun colSums(random_vars) / 1000 to obtain \\(E[\\text{age}] = 33.471\\) and \\(E[\\text{income}] = 3510.731\\)\nRun var(random_vars[1]) and var(random_vars[2]) to obtain \\(Var[age] = 340.6078\\) and \\(Var[income] = 8625646\\)\nTake the square roots of the variances to obtain \\(\\sigma_{age} = 18.456\\) and \\(\\sigma_{income} = 2936.945\\)\n\n\n\n2 \n\nIt does not make sense to compare the standard deviations alone, since the numbers are different orders of magnitude. That is, a difference in age by 50 is a significant, but a difference in income by 50 is not. One could scale the standard deviation in some way with respect to the expected value for a better comparison.\n\n\n3 \n\n\nRunning cov(random_vars) shows the covariance is \\(29700.15\\)\nRunning cor(random_vars) shows the correlation is \\(0.548\\)\n\n\n\n4 \n\nCorrelation is easier to interpret, because it is scaled from -1 to 1. Thus it is easily comparable between different sets of variables. However the scale of the covariance depends of the scale of the variables themselves.\n\n\n5 \n\n\nRun the following code\n\nchildren = filter(random_vars, age&lt;=18)\ncolSums(children[2]) / nrow(children)\nto obtain \\(E[\\text{income}|\\text{age} \\le 18] = 389.61\\)\n\nRun\n\nmiddle_age = filter(random_vars, age &gt;= 18 & age &lt; 65)\ncolSums(middle_age[2]) / nrow(middle_age)\nto obtain \\(E[\\text{income}|\\text{age} \\in [18,65)] = 4685.73\\)\n\nRun\n\nold = filter(random_vars, age &gt;= 65)\ncolSums(old[2]) / nrow(old)\nto obtain \\(E[\\text{income}|\\text{age} \\ge 65] = 1777.24\\)"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "1 \n\nhere\n\n\n2  \n\nlibrary(tidyverse)\ncustomer_sat &lt;- readRDS('~/College/TUHH/causual_data_science/data/customer_sat.rds')\nlm_sat_fol &lt;- lm(satisfaction ~ follow_ups, customer_sat)\nsummary(lm_sat_fol)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n\n\nlm_sat_fol_sub &lt;- lm(satisfaction ~ . , customer_sat)\nsummary(lm_sat_fol_sub)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ ., data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\n\n\n3 \n\nThe coefficient for the regression of satisfaction on follow_ups is negative, yet the coefficients of satisfaction on follow_ups when considering subscription type are all positive. This illustrates that without considering subscription type, one could incorrectly assume that more follow ups indicate less satisfaction. However when one considers subscription type as well, it becomes clearer that the different groupings of satisfaction seem to be influenced more by subscription type than follow ups. Within these groupings more follow ups actually does improve user satisfaction.\n\n\n4 \n\n\nsatisfy_followup &lt;- ggplot(customer_sat, aes(x = follow_ups, y = satisfaction)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\nsatisfy_followup\n\n\n\n\n\n\n\nsatisfy_followup_cond &lt;- ggplot(customer_sat, aes(x = follow_ups, y = satisfaction, color = subscription)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(legend.position = \"right\")\nsatisfy_followup_cond"
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "1 \n\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\n\nschooling_2 &lt;- dagify(\n  parking ~ location,\n  sales ~ location,\n  sales ~ parking,\n  coords = list(x = c(parking = 1, sales = 3, location = 2),\n                y = c(parking = 1, sales = 1, location = 2))\n)\n\nggdag(schooling_2, text = F) +\n  geom_dag_text(color = NA) +\n  geom_dag_edges(edge_color = \"blue\") +\n  geom_dag_label_repel(aes(label = name))\n\n\n\n\n\n\n\n\nLocation is a confounder on parking spots and sales. Thus one cannot simply say that the treatment (parking spots) have a causal effect on the outcome (sales).\n\n\n2  \n\nlibrary(tidyverse)\ncustomer_sat &lt;- readRDS('~/College/TUHH/causual_data_science/data/customer_sat.rds')\nlm_sat_fol &lt;- lm(satisfaction ~ follow_ups, customer_sat)\nsummary(lm_sat_fol)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n\n\nlm_sat_fol_sub &lt;- lm(satisfaction ~ . , customer_sat)\nsummary(lm_sat_fol_sub)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ ., data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\n\n\n3 \n\nThe coefficient for the regression of satisfaction on follow_ups is negative, yet the coefficients of satisfaction on follow_ups when considering subscription type are all positive. This illustrates that without considering subscription type, one could incorrectly assume that more follow ups indicate less satisfaction. However when one considers subscription type as well, it becomes clearer that the different groupings of satisfaction seem to be influenced more by subscription type than follow ups. Within these groupings more follow ups actually does improve user satisfaction.\n\n\n4 \n\n\nsatisfy_followup &lt;- ggplot(customer_sat, aes(x = follow_ups, y = satisfaction)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\nsatisfy_followup\n\n\n\n\n\n\n\nsatisfy_followup_cond &lt;- ggplot(customer_sat, aes(x = follow_ups, y = satisfaction, color = subscription)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(legend.position = \"right\")\nsatisfy_followup_cond"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "library(tidyverse)\nabtest &lt;- readRDS('~/College/TUHH/causual_data_science/data/abtest_online.rds')\n\n\n1  \n\nprev_visit &lt;- \n  ggplot(abtest, \n         aes(x = chatbot, \n             y = previous_visit, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Previous Visits\", title = \"Difference in Previous Visits\")\n\npurchase_amt &lt;- \n  ggplot(abtest, \n         aes(x = chatbot, \n             y = purchase_amount, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Outcome\", title = \"Difference in outcome\")\n\npurchase &lt;- \n  ggplot(abtest, \n         aes(x = chatbot, \n             y = purchase, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Outcome\", title = \"Difference in outcome\")\n\nprev_visit\n\n\n\n\n\n\n\npurchase_amt\n\n\n\n\n\n\n\npurchase\n\n\n\n\n\n\n\n\n\n\n2 \n\n\nlm_amt &lt;- lm(purchase_amount ~ chatbot, abtest)\nsummary(lm_amt)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = abtest)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbotTRUE  -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\nThere appears to be a negative correlation between chatbot and sales.\n\n\n3 \n\n\nlm_cate &lt;- lm(purchase_amount ~ chatbot * previous_visit + chatbot * mobile_device, abtest)\nsummary(lm_cate)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * previous_visit + chatbot * \n#&gt;     mobile_device, data = abtest)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -22.057 -15.928  -7.419  12.804  65.333 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    17.2052     1.2982  13.253  &lt; 2e-16 ***\n#&gt; chatbotTRUE                   -11.1064     1.8434  -6.025 2.38e-09 ***\n#&gt; previous_visit                 -0.1030     0.3749  -0.275 0.783562    \n#&gt; mobile_deviceTRUE              -0.8791     1.7823  -0.493 0.621943    \n#&gt; chatbotTRUE:previous_visit      2.0978     0.5781   3.629 0.000299 ***\n#&gt; chatbotTRUE:mobile_deviceTRUE   0.2044     2.5148   0.081 0.935229    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.49 on 994 degrees of freedom\n#&gt; Multiple R-squared:  0.05495,    Adjusted R-squared:  0.0502 \n#&gt; F-statistic: 11.56 on 5 and 994 DF,  p-value: 7.177e-11\n\n\n\n\n4 \n\n\nlm_boolean &lt;- lm(purchase ~ chatbot * previous_visit + chatbot * mobile_device, abtest)\nsummary(lm_boolean)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase ~ chatbot * previous_visit + chatbot * \n#&gt;     mobile_device, data = abtest)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.6618 -0.4708 -0.2035  0.4992  0.8601 \n#&gt; \n#&gt; Coefficients:\n#&gt;                                 Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    0.5096692  0.0326912  15.590  &lt; 2e-16 ***\n#&gt; chatbotTRUE                   -0.3572109  0.0464210  -7.695 3.40e-14 ***\n#&gt; previous_visit                -0.0044165  0.0094401  -0.468    0.640    \n#&gt; mobile_deviceTRUE             -0.0129354  0.0448818  -0.288    0.773    \n#&gt; chatbotTRUE:previous_visit     0.0680830  0.0145576   4.677 3.32e-06 ***\n#&gt; chatbotTRUE:mobile_deviceTRUE  0.0003328  0.0633288   0.005    0.996    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4657 on 994 degrees of freedom\n#&gt; Multiple R-squared:  0.08604,    Adjusted R-squared:  0.08144 \n#&gt; F-statistic: 18.71 on 5 and 994 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "1 Assignment I\n\\[\\begin{gather}\nP(T \\cap S) =  0.2 * 0.3 = 0.06 \\\\\nP(T \\cap \\overline{S}) =  0.6 * 0.7 = 0.42 \\\\\nP(\\overline{T} \\cap S) =  0.8 * 0.3 = 0.24 \\\\\nP(\\overline{T} \\cap \\overline{S}) =  0.4 * 0.7 = 0.28 \\\\\nP(T \\cap S) + P(T \\cap \\overline{S}) + P(\\overline{T} \\cap S) + P(\\overline{T} \\cap \\overline{S}) = 1\n\\end{gather}\\]\n\n\n2 Assignment II\n\nPercentage of customers using all three devices:\n\n0.5%\n\nPercentage of customers using at least 2 devices:\n\n\\(7.3 + 3.3 + 8.8 + 0.5 = 19.9\\text{%}\\)\n\nPercentage of customers using only one device:\n\n\\(27.8 + 42.3 + 10.0 = 80.1\\text{%}\\)\n\n\n\n\n3 Assignment III\nFrom the given info we have \\(P(B|A) = 0.97\\) and \\(P(B|\\overline{A}) = 0.01\\)\nand thus: \\(P(\\overline{B}|\\overline{A}) = 0.99\\)\n\\(P(B) = P(B|A) * P(A) + P(B|\\overline{A}) * P(\\overline{A}) = 0.97 * 0.04 + 0.01 * 0.96 = 0.0484\\)\n\\(P( \\overline{A}|B ) = \\frac{P(B|\\overline{A}) P(\\overline{A})} {P(B)} = \\frac{0.01 * 0.96}{0.0484} = 0.1983\\)\n\\(P(A|B) = \\frac{P(B|A) P(A)}{P(B)} = \\frac{0.97 * 0.04}{0.0484} = 0.8017\\)\nThese probabilities add up to 1, as they should"
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "5.1 Header 2",
    "text": "5.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "1 \n\nThe data has 181 rows and 22 columns\n\n\n2 \n\nEntries are of data types, numeric and character. Numbers are numeric data type. Strings are character data type.\n\n\n3 \n\nRun\nmodel &lt;- lm(price ~ ., car_prices)\nsummary(model)\nin order to create a linear model comparing the effect of all other parameters on price. this obtains the following:\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \naspirationturbo        1846.206   1041.391   1.773 0.078386 .  \ndoornumbertwo           242.523    571.929   0.424 0.672172    \ncarbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \ncarbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \ncarbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \ncarbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \ndrivewheelfwd          -504.564   1076.623  -0.469 0.640030    \ndrivewheelrwd           -15.446   1268.070  -0.012 0.990299    \nenginelocationrear     6643.492   2572.275   2.583 0.010806 *  \nwheelbase               -30.197     92.776  -0.325 0.745294    \ncarlength               -29.740     51.672  -0.576 0.565824    \ncarwidth                731.819    244.533   2.993 0.003258 ** \ncarheight               123.195    134.607   0.915 0.361617    \ncurbweight                2.612      1.781   1.467 0.144706    \nenginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \nenginetypel             978.748   1786.384   0.548 0.584619    \nenginetypeohc          3345.252    933.001   3.585 0.000461 ***\nenginetypeohcf          972.919   1625.631   0.598 0.550462    \nenginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\ncylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\ncylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\ncylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \ncylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \ncylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \nenginesize              125.934     26.541   4.745 5.00e-06 ***\nfuelsystem2bbl          177.136    883.615   0.200 0.841400    \nfuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \nfuelsystemmpfi          359.278   1001.529   0.359 0.720326    \nfuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \nfuelsystemspfi          514.766   2499.229   0.206 0.837107    \nboreratio             -1306.740   1642.221  -0.796 0.427516    \nstroke                -4527.137    922.732  -4.906 2.49e-06 ***\ncompressionratio       -737.901    555.960  -1.327 0.186539    \nhorsepower               10.293     22.709   0.453 0.651035    \npeakrpm                   2.526      0.634   3.983 0.000108 ***\ncitympg                 -90.352    166.647  -0.542 0.588538    \nhighwaympg              154.858    167.148   0.926 0.355761    \naccording to this model engine type/size, cylinder number, stroke and peak rpm are the most significant factors in car price (as they have the lowest p values).\n\n\n4 \n\nMaking a new linear model with just price and engine size (an integer value, which in our data set falls between \\(61\\) and \\(326\\))\nenginesize_price &lt;- lm(price ~ enginesize, car_prices)\nsummary(enginesize_price)\nwe obtain the following:\nResiduals:\n   Min       1Q   Median       3Q      Max \n-10818.6  -1969.6   -168.6   1494.0  14393.9 \n\nCoefficients:\n           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -8622.296    873.535  -9.871   &lt;2e-16 ***\nenginesize    170.064      6.523  26.073   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3694 on 179 degrees of freedom\nMultiple R-squared:  0.7916,    Adjusted R-squared:  0.7904 \nF-statistic: 679.8 on 1 and 179 DF,  p-value: &lt; 2.2e-16\nthe estimate value of \\(170.064\\) tells us that for every single unit the engine size increases the price of the car typically increase by \\(\\$170.06\\).\nlooking at the p value, which is practically \\(0\\), we can conclude that enginesize does effect price in a statistically significant manner. This significance is further displayed by the standard error of only \\(6.523\\), on a variable with an estimate of \\(170.064\\) and an expected value of \\(127.14\\). Lastly we can see that our adjusted \\(R^2\\) is close to one, indicating a high amount of variance in price can be attributed to variance in engine size.\n\n\n5 \n\ncar_prices_updated &lt;- car_prices %&gt;% mutate(seat_heating = TRUE)\nseatheating_price &lt;- lm(price ~ seat_heating, car_prices_updated)\nsummary(seatheating_price)\nCoefficients: (1 not defined because of singularities)\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       12999.4      599.7   21.68   &lt;2e-16 ***\nseat_heatingTRUE       NA         NA      NA       NA \nThe coefficient is not defined. Because all values of the seat_heating variable are the same, there is no way to determine how a change in seat_heating corresponds to a change in price (or in anything)."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#section",
    "href": "content/01_journal/02_statistics.html#section",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Run colSums(random_vars) / 1000 to obtain \\(E[\\text{age}] = 33.471\\) and \\(E[\\text{income}] = 3510.731\\)\nRun var(random_vars[1]) and var(random_vars[2]) to obtain \\(Var[age] = 340.6078\\) and \\(Var[income] = 8625646\\)\nTake the square roots of the variances to obtain \\(\\sigma_{age} = 18.456\\) and \\(\\sigma_{income} = 2936.945\\)"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#q1",
    "href": "content/01_journal/02_statistics.html#q1",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Expected Value"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#d",
    "href": "content/01_journal/02_statistics.html#d",
    "title": "Statistical Concepts",
    "section": "2 d",
    "text": "2 d"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#problem-1",
    "href": "content/01_journal/02_statistics.html#problem-1",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Run colSums(random_vars) / 1000 to obtain \\(E(\\text{age}) = 33.471\\) and \\(E(\\text{income}) = 3510.731\\)\nRun var(random_vars[1]) and var(random_vars[2]) to obtain \\(Var(age) = 340.6078\\) and \\(Var(income) = 8625646\\)\nTake the square roots of the variances to obtain \\(\\sigma_{age} = 18.456\\) and \\(\\sigma_{income} = 2936.945\\)"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#problem-2",
    "href": "content/01_journal/02_statistics.html#problem-2",
    "title": "Statistical Concepts",
    "section": "2 Problem 2",
    "text": "2 Problem 2\nIt does not make sense to compare the standard deviations alone, since the numbers are different orders of magnitude. That is, a difference in age by 50 is a significant, but a difference in income by 50 is not. One could scale the standard deviation in some way with respect to the expected value for a better comparison."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#problem-3",
    "href": "content/01_journal/02_statistics.html#problem-3",
    "title": "Statistical Concepts",
    "section": "3 Problem 3",
    "text": "3 Problem 3\n\nRunning cov(random_vars) shows the covariance is \\(29700.15\\)\nRunning cor(random_vars) shows the correlation is \\(0.548\\)"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#problem-4",
    "href": "content/01_journal/02_statistics.html#problem-4",
    "title": "Statistical Concepts",
    "section": "4 Problem 4",
    "text": "4 Problem 4\nCorrelation is easier to interpret, because it is scaled from -1 to 1. Thus it is easily comparable between different sets of variables. However the scale of the covariance depends of the scale of the variables themselves."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#problem-5",
    "href": "content/01_journal/02_statistics.html#problem-5",
    "title": "Statistical Concepts",
    "section": "5 Problem 5",
    "text": "5 Problem 5\n\n\n\nchildren = filter(random_vars, age&lt;=18)\ncolSums(children[2]) / nrow(children)\nyields \\(E[\\text{income}|\\text{age} \\le 18] = 389.61\\)"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#section-1",
    "href": "content/01_journal/02_statistics.html#section-1",
    "title": "Statistical Concepts",
    "section": "2 \n",
    "text": "2 \n\nIt does not make sense to compare the standard deviations alone, since the numbers are different orders of magnitude. That is, a difference in age by 50 is a significant, but a difference in income by 50 is not. One could scale the standard deviation in some way with respect to the expected value for a better comparison."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#section-2",
    "href": "content/01_journal/02_statistics.html#section-2",
    "title": "Statistical Concepts",
    "section": "3 \n",
    "text": "3 \n\n\nRunning cov(random_vars) shows the covariance is \\(29700.15\\)\nRunning cor(random_vars) shows the correlation is \\(0.548\\)"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#section-3",
    "href": "content/01_journal/02_statistics.html#section-3",
    "title": "Statistical Concepts",
    "section": "4 \n",
    "text": "4 \n\nCorrelation is easier to interpret, because it is scaled from -1 to 1. Thus it is easily comparable between different sets of variables. However the scale of the covariance depends of the scale of the variables themselves."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#section-4",
    "href": "content/01_journal/02_statistics.html#section-4",
    "title": "Statistical Concepts",
    "section": "5 \n",
    "text": "5 \n\n\nRun the following code\n\nchildren = filter(random_vars, age&lt;=18)\ncolSums(children[2]) / nrow(children)\nto obtain \\(E[\\text{income}|\\text{age} \\le 18] = 389.61\\)\n\nRun\n\nmiddle_age = filter(random_vars, age &gt;= 18 & age &lt; 65)\ncolSums(middle_age[2]) / nrow(middle_age)\nto obtain \\(E[\\text{income}|\\text{age} \\in [18,65)] = 4685.73\\)\n\nRun\n\nold = filter(random_vars, age &gt;= 65)\ncolSums(old[2]) / nrow(old)\nto obtain \\(E[\\text{income}|\\text{age} \\ge 65] = 1777.24\\)"
  }
]